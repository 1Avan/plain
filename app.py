import pyecharts.charts
import streamlit as st
import streamlit_echarts as st_echarts
import requests
from bs4 import BeautifulSoup
import jieba
from pyecharts.charts import Bar, WordCloud
from pyecharts import options as opts
import re # æ­£åˆ™è¡¨è¾¾å¼åº“
from nltk.corpus import stopwords # åœç”¨è¯åº“

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»æ‰æ ‡ç‚¹ç¬¦å·
def remove_punctuations(text):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»æ‰æ ‡ç‚¹ç¬¦å·
    cleaned_text = re.sub(r'[^\w\s]', '', text)
    return cleaned_text

def crawlingFn(url):
    # å‘é€GETè¯·æ±‚å¹¶è·å–å“åº”
    response = requests.get(url)
    # ç¡®å®šç¼–ç 
    encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None
    # ä½¿ç”¨BeautifulSoupè§£æå“åº”æ–‡æœ¬
    soup = BeautifulSoup(response.content, 'html.parser', from_encoding=encoding)
    # è·å–æ–‡æœ¬å†…å®¹
    text_content = soup.text
    # å¯¹æ–‡æœ¬å†…å®¹æ¸…æ´—
    text_content = remove_punctuations(text_content)
    text_content = remove_stopwords(text_content)
    return text_content
def page_home():
    # è¿™æ˜¯ä¸»é¡µé¢
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url= st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        bar = Bar()
        val = list(map(int, word_counts_20.values()))
        wordList = list(word_counts_20.keys())
        bar.add_xaxis(wordList)
        bar.add_yaxis("å…³é”®è¯", val)
        # è®¾ç½® x è½´æ ‡ç­¾æ—‹è½¬è§’åº¦ä¸º 45 åº¦
        bar.set_global_opts(
            xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=45))
        )
        # ä½¿ç”¨ st_echarts.st_pyecharts() æ–¹æ³•å°†å›¾è¡¨æ¸²æŸ“åˆ° Streamlit ä¸­
        st_echarts.st_pyecharts(bar)
def page_ciyun():
    # è¯äº‘æ•°æ®
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url = st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        # [()]
        word_list = [(x,y) for x,y in word_counts_20.items()]
        wordcloud = WordCloud()
        wordcloud.add("", word_list, word_size_range=[20, 100])
        st_echarts.st_pyecharts(wordcloud)
def page_pie():
    # é¥¼çŠ¶å›¾é¡µé¢
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url = st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        # [()]
        word_list = [(x,y) for x,y in word_counts_20.items()]
        pie = pyecharts.charts.Pie()
        pie.add("",word_list, radius=["40%", "75%"])
        pie.set_global_opts(title_opts=opts.TitleOpts(title=""))
        st_echarts.st_pyecharts(pie)
def page_broken():
    # è¿™æ˜¯æŠ˜çº¿å›¾
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url= st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        val = list(map(int, word_counts_20.values()))
        wordList = list(word_counts_20.keys())
        line = pyecharts.charts.Line()
        line.add_xaxis(wordList)
        line.add_yaxis("å…³é”®è¯",val)
        # è®¾ç½® x è½´æ ‡ç­¾æ—‹è½¬è§’åº¦ä¸º 45 åº¦
        line.set_global_opts(
            xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=45))
        )
        st_echarts.st_pyecharts(line)
def page_point():
    # è¿™æ˜¯æŠ˜çº¿å›¾
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url= st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        val = list(map(int, word_counts_20.values()))
        wordList = list(word_counts_20.keys())
        size_data = [10, 20, 30, 40, 50, 60]
        es = pyecharts.charts.EffectScatter()
        es.add_xaxis(wordList)
        es.add_yaxis("å…³é”®è¯",val,symbol_size=size_data)
        # è®¾ç½® x è½´æ ‡ç­¾æ—‹è½¬è§’åº¦ä¸º 45 åº¦
        es.set_global_opts(
            xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=45))
        )
        st_echarts.st_pyecharts(es)
def main():
    # è®¾ç½®åˆå§‹é¡µé¢ä¸ºHome
    session_state = st.session_state
    session_state['page'] = 'æ¡å½¢å›¾'
    # å¯¼èˆªæ 
    page = st.sidebar.radio('å¯¼èˆªæ ', ['æ¡å½¢å›¾', 'è¯äº‘','é¥¼çŠ¶å›¾','æŠ˜çº¿å›¾','æ•£ç‚¹å›¾'])

    if page == 'æ¡å½¢å›¾':
        # åœ¨Homeé¡µé¢ä¸­æ˜¾ç¤ºæ•°æ®å’ŒåŠŸèƒ½ç»„ä»¶
        page_home()

    elif page == 'è¯äº‘':
        # åœ¨Abouté¡µé¢ä¸­æ˜¾ç¤ºæ•°æ®å’ŒåŠŸèƒ½ç»„ä»¶
        page_ciyun()
    elif page == 'é¥¼çŠ¶å›¾':
        # åœ¨Abouté¡µé¢ä¸­æ˜¾ç¤ºæ•°æ®å’ŒåŠŸèƒ½ç»„ä»¶
        page_pie()
    elif page == 'æŠ˜çº¿å›¾':
        page_broken()
    elif page == 'æ•£ç‚¹å›¾':
        page_point()

if __name__ == '__main__':
    main()