import pyecharts.charts
import pypinyin
import streamlit as st
import streamlit_echarts as st_echarts
import requests
from bs4 import BeautifulSoup
import jieba
from pyecharts.charts import Bar, WordCloud,Map
# steamlitä¸­åµŒå…¥pyechatså‰ç«¯ä»£ç 
import streamlit.components.v1 as components
from pyecharts import options as opts
# æ±‰å­—è½¬æ‹¼éŸ³
from pypinyin import pinyin, Style
import gopup as gp
import re # æ­£åˆ™è¡¨è¾¾å¼åº“
# from nltk.corpus import stopwords # åœç”¨è¯åº“

def remove_stopwords(text):
    # stop_words = set(stopwords.words('english'))
    stop_words = {'my', 'not', 'couldn', "mustn't", 'and', 'why', "weren't", 'its', 'same', 'hasn', 'again', 'being', "you'd", 'hers', 'don', "wasn't", 'more', "isn't", 'when', 'ma', 'were', 't', 'by', 're', "couldn't", 'we', 'that', "hadn't", 'she', 'down', 's', 'themselves', 'each', 'because', 'having', "you're", 'herself', 'a', 'those', 'them', 'above', 'how', 'only', 'shouldn', 've', 'itself', 'be', 'out', 'up', 'until', 'whom', 'yours', 'did', 'our', 'through', 'below', 'won', "won't", 'nor', 'now', 'off', 'while', "should've", 'wouldn', 'll', 'needn', "mightn't", 'didn', 'hadn', 'an', "wouldn't", 'from', 'in', 'all', 'yourselves', 'both', 'after', 'he', 'few', "you've", 'at', 'these', 'him', "aren't", "haven't", 'his', 'has', 'you', 'myself', 'aren', 'with', 'it', 'will', 'any', "shan't", 'than', 'some', 'haven', 'mustn', "shouldn't", 'theirs', 'been', 'their', 'about', 'on', "you'll", 'm', 'into', 'himself', 'yourself', 'doesn', 'are', 'such', 'your', 'against', 'to', 'mightn', 'doing', 'further', 'over', 'as', 'they', 'during', 'so', 'there', 'between', 'which', 'once', 'me', 'had', 'here', 'under', 'most', 'can', 'but', 'before', 'wasn', "that'll", 'd', 'just', "she's", "it's", 'other', 'have', 'no', 'i', "didn't", 'her', "don't", 'ours', 'very', 'the', 'should', 'too', "needn't", 'if', 'of', 'was', 'isn', 'own', 'what', 'where', 'ourselves', 'or', 'this', 'ain', 'then', 'for', 'weren', 'do', 'who', 'is', "hasn't", "doesn't", 'shan', 'am', 'o', 'y', 'does'}
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»æ‰æ ‡ç‚¹ç¬¦å·
def remove_punctuations(text):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»æ‰æ ‡ç‚¹ç¬¦å·
    cleaned_text = re.sub(r'[^\w\s]', '', text)
    return cleaned_text

def crawlingFn(url):
    # å‘é€GETè¯·æ±‚å¹¶è·å–å“åº”
    response = requests.get(url)
    # ç¡®å®šç¼–ç 
    encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None
    # ä½¿ç”¨BeautifulSoupè§£æå“åº”æ–‡æœ¬
    soup = BeautifulSoup(response.content, 'html.parser', from_encoding=encoding)
    # è·å–æ–‡æœ¬å†…å®¹
    text_content = soup.text
    # å¯¹æ–‡æœ¬å†…å®¹æ¸…æ´—
    text_content = remove_punctuations(text_content)
    text_content = remove_stopwords(text_content)
    return text_content
def page_home():
    # è¿™æ˜¯ä¸»é¡µé¢
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url= st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        bar = Bar()
        val = list(map(int, word_counts_20.values()))
        wordList = list(word_counts_20.keys())
        bar.add_xaxis(wordList)
        bar.add_yaxis("å…³é”®è¯", val)
        # è®¾ç½® x è½´æ ‡ç­¾æ—‹è½¬è§’åº¦ä¸º 45 åº¦
        bar.set_global_opts(
            xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=45))
        )
        # ä½¿ç”¨ st_echarts.st_pyecharts() æ–¹æ³•å°†å›¾è¡¨æ¸²æŸ“åˆ° Streamlit ä¸­
        st_echarts.st_pyecharts(bar)
def page_ciyun():
    # è¯äº‘æ•°æ®
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url = st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        # [()]
        word_list = [(x,y) for x,y in word_counts_20.items()]
        wordcloud = WordCloud()
        wordcloud.add("", word_list, word_size_range=[20, 100])
        st_echarts.st_pyecharts(wordcloud)
def page_pie():
    # é¥¼çŠ¶å›¾é¡µé¢
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url = st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        # [()]
        word_list = [(x,y) for x,y in word_counts_20.items()]
        pie = pyecharts.charts.Pie()
        pie.add("",word_list, radius=["40%", "75%"])
        pie.set_global_opts(title_opts=opts.TitleOpts(title=""))
        st_echarts.st_pyecharts(pie)
def page_broken():
    # è¿™æ˜¯æŠ˜çº¿å›¾
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url= st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        val = list(map(int, word_counts_20.values()))
        wordList = list(word_counts_20.keys())
        line = pyecharts.charts.Line()
        line.add_xaxis(wordList)
        line.add_yaxis("å…³é”®è¯",val)
        # è®¾ç½® x è½´æ ‡ç­¾æ—‹è½¬è§’åº¦ä¸º 45 åº¦
        line.set_global_opts(
            xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=45))
        )
        st_echarts.st_pyecharts(line)
def page_point():
    # è¿™æ˜¯æŠ˜çº¿å›¾
    st.title('æ¬¢è¿ä½¿ç”¨ç½‘é¡µè¯é¢‘å¯è§†åŒ–å·¥å…·! ğŸ‘‹')
    input_url= st.text_input("Enter URL:")
    if input_url.strip() == "":
        return
    else:
        text = crawlingFn(input_url)
        words = jieba.lcut(text)  # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        word_counts = {}
        # è·å–è¯é¢‘å­—å…¸
        for word in words:
            if len(word) == 1:
                continue
            else:
                word_counts[word] = word_counts.get(word, 0) + 1
        # å­—å…¸æŒ‰å€¼ä»å¤§åˆ°å°å–å‰20ä¸ª
        word_counts_20 = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20])
        val = list(map(int, word_counts_20.values()))
        wordList = list(word_counts_20.keys())
        size_data = [10, 20, 30, 40, 50, 60]
        es = pyecharts.charts.EffectScatter()
        es.add_xaxis(wordList)
        es.add_yaxis("å…³é”®è¯",val,symbol_size=size_data)
        # è®¾ç½® x è½´æ ‡ç­¾æ—‹è½¬è§’åº¦ä¸º 45 åº¦
        es.set_global_opts(
            xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=45))
        )
        st_echarts.st_pyecharts(es)
def page_movie():
    df_index = gp.douban_movie_list()
    df_index2 = gp.douban_week_praise_list()
    dataList = list()
    for item in df_index2.iterrows():
        temList = list()
        temList.append(item[1][0])
        temList.append(item[1][3])
        temList.append(item[1][2])
        temList.append(item[1][1])
        dataList.append(temList)
    dataList = sorted(dataList, key=lambda x: x[1])
    titleList = list()
    scoreList = list()
    imgList = list()
    linkList = list()
    for item in df_index.iterrows():
        # print(item[1][2],item[1][0],item[1][5])
        titleList.append(item[1][2])
        scoreList.append(float(item[1][0]))
        imgList.append(item[1][4])
        linkList.append(item[1][5])
    bar = Bar()
    bar.add_xaxis(titleList)
    bar.add_yaxis("è±†ç“£è¯„åˆ†", scoreList)
    # è®¾ç½® x è½´æ ‡ç­¾æ—‹è½¬è§’åº¦ä¸º 45 åº¦
    bar.set_global_opts(
        xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=20))
    )
    st.title("è±†ç“£æ–°ç‰‡æ¦œ")
    st_echarts.st_pyecharts(bar)
    expander_1 = st.expander('è§‚çœ‹å…¥å£ï¼š')
    # å¾ªç¯éå†å›¾ç‰‡åˆ—è¡¨å¹¶æ˜¾ç¤ºå›¾ç‰‡
    for i in range(len(imgList)):
        # unsafe_allow_htmlå…è®¸writeæ’å…¥htmlæ ‡ç­¾
        expander_1.write(f"<a href={linkList[i]}>{titleList[i]}</a>", unsafe_allow_html=True)
    st.title("è±†ç“£ä¸€å‘¨å£ç¢‘æ¦œ")
    headers = ['å½±ç‰‡', 'æ’å', 'è¶‹åŠ¿',"é“¾æ¥"]
    # ç¼–å†™HTMLä»£ç ï¼ŒåŒ…æ‹¬è¡¨å¤´å’Œæ•°æ®è¡Œ
    table_html = f"<table><thead><tr>{''.join(f'<th>{header}</th>' for header in headers)}</tr></thead><tbody>"
    for row in dataList:
        table_html += f"<tr>{''.join(f'<td>{data}</td>' for data in row)}</tr>"
    table_html += "</tbody></table>"
    # ä½¿ç”¨st.write()å‡½æ•°æ˜¾ç¤ºè‡ªå®šä¹‰è¡¨æ ¼
    st.write(table_html, unsafe_allow_html=True)
def page_weather():
    # openweathermap api-key
    api_key = "12b2817fbec86915a6e9b4dbbd3d9036"

    def get_weather(city):
        url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
        response = requests.get(url)
        data = response.json()
        return data

    def get_temperatureAll():
        # ä½¿ç”¨pypinyinåº“å°†ä¸­æ–‡çœä»½åç§°è½¬æ¢ä¸ºå¯¹åº”çš„æ‹¼éŸ³å½¢å¼ï¼Œå¹¶å»æ‰ä»£è¡¨â€œçœâ€ã€â€œå¸‚â€ç­‰çš„åç¼€ï¼Œæœ€ç»ˆå¾—åˆ°äº†ä¸€ä¸ªåŒ…å«æ‰€æœ‰çœä»½æ‹¼éŸ³åç§°çš„åˆ—è¡¨
        provinces = ['åŒ—äº¬å¸‚', 'å¤©æ´¥å¸‚', 'æ²³åŒ—çœ', 'å±±è¥¿çœ', 'å‰æ—çœ', 'é»‘é¾™æ±Ÿçœ', 'ä¸Šæµ·å¸‚', 'æ±Ÿè‹çœ', 'æµ™æ±Ÿçœ', 'å®‰å¾½çœ', 'ç¦å»ºçœ', 'æ±Ÿè¥¿çœ', 'å±±ä¸œçœ', 'æ²³å—çœ',
                     'æ¹–åŒ—çœ', 'æ¹–å—çœ', 'å¹¿ä¸œçœ', 'æµ·å—çœ', 'é‡åº†å¸‚', 'å››å·çœ', 'è´µå·çœ', 'è¥¿è—è‡ªæ²»åŒº', 'é™•è¥¿çœ', 'ç”˜è‚ƒçœ', 'é’æµ·çœ', 'æ–°ç–†', 'é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº',
                     'å°æ¹¾çœ']
        # provinces = ['åŒ—äº¬å¸‚', 'å¤©æ´¥å¸‚', 'æ²³åŒ—çœ', 'å±±è¥¿çœ', 'å‰æ—çœ', 'é»‘é¾™æ±Ÿçœ',]
        temperature_data = []
        for i in range(len(provinces)):
            # å»é™¤åç¼€
            province_name = provinces[i].replace('è‡ªæ²»åŒº', '').replace('ç‰¹åˆ«è¡Œæ”¿åŒº', '').replace('çœ', '').replace('å¸‚', '')
            # å°†ä¸­æ–‡è½¬ä¸ºæ‹¼éŸ³
            city_pinyin = ''.join(p[0] for p in pinyin(province_name, style=Style.NORMAL))
            temperature = get_weather(city_pinyin)["main"]["temp"]
            # è¦è·å–æ¸©åº¦çš„åŸå¸‚åˆ—è¡¨
            temperature_data.append((provinces[i], temperature))
        return temperature_data

    def visualize_weather(data):
        temperature = data["main"]["temp"]
        humidity = data["main"]["humidity"]
        wind_speed = data["wind"]["speed"]
        bar = (
            Bar()
            .add_xaxis(["æ¸©åº¦", "æ¹¿åº¦", "é£é€Ÿ"])
            .add_yaxis("å¤©æ°”æ•°æ®", [temperature, humidity, wind_speed])
            .set_global_opts(title_opts=opts.TitleOpts(title="ä¸­å›½å¤©æ°”å¯è§†åŒ–"))
        )
        return bar

    def main():
        st.set_option('deprecation.showPyplotGlobalUse', False)
        st.title("ä¸­å›½å¤©æ°”å¯è§†åŒ–")
        city = st.text_input("è¯·è¾“å…¥åŸå¸‚åï¼š")
        if st.button("æŸ¥è¯¢"):
            pinyin_list = pypinyin.lazy_pinyin(city)
            pinyin_city = ''.join(pinyin_list)
            weather_data = get_weather(pinyin_city)
            bar = visualize_weather(weather_data)
            st_echarts.st_pyecharts(bar)
        # æ•°æ®ç¤ºä¾‹
        # data = [("åŒ—äº¬å¸‚", 111)]

        data = get_temperatureAll()

        map_chart = (
            Map()
            .add("æ•°æ®ç³»åˆ—åç§°", data, "china")
            .set_series_opts(label_opts=opts.LabelOpts(is_show=False))
            .set_global_opts(
                title_opts=opts.TitleOpts(title="ä¸­å›½å„åœ°æ¸©åº¦"),
                visualmap_opts=opts.VisualMapOpts(
                    max_=300,
                    min_=0,
                    is_piecewise=True,
                    pieces=[
                        {"min": -273.15, "max": 0},
                        {"min": 0, "max": 100},
                        {"min": 101, "max": 200},
                        {"min": 201, "max": 300},
                    ]

                ),
                tooltip_opts=opts.TooltipOpts(formatter="{b}: {c}"),
            )
        )

        cities = [
            "åŒ—äº¬",
            "ä¸Šæµ·",
            # å…¶ä»–åŸå¸‚...
        ]

        for city in cities:
            try:
                weather_data = get_weather(city)
                temperature = weather_data["main"]["temp"]
                map_chart.add("", [(city, temperature)])
            except:
                pass

        htmlcode = map_chart.render_embed()  # åµŒå…¥å¼æ¸²æŸ“
        components.html(htmlcode, width=1000, height=600)
    main()


def main():
    # è®¾ç½®åˆå§‹é¡µé¢ä¸ºHome
    session_state = st.session_state
    session_state['page'] = 'æ¡å½¢å›¾'
    # å¯¼èˆªæ 
    page = st.sidebar.selectbox('å¯¼èˆªæ ', ['æ¡å½¢å›¾', 'è¯äº‘','é¥¼çŠ¶å›¾','æŠ˜çº¿å›¾','æ•£ç‚¹å›¾','å½±è§†æ¨è','å›½å†…å¤©æ°”'])

    if page == 'æ¡å½¢å›¾':
        # åœ¨Homeé¡µé¢ä¸­æ˜¾ç¤ºæ•°æ®å’ŒåŠŸèƒ½ç»„ä»¶
        page_home()
    elif page == 'è¯äº‘':
        # åœ¨Abouté¡µé¢ä¸­æ˜¾ç¤ºæ•°æ®å’ŒåŠŸèƒ½ç»„ä»¶
        page_ciyun()
    elif page == 'é¥¼çŠ¶å›¾':
        # åœ¨Abouté¡µé¢ä¸­æ˜¾ç¤ºæ•°æ®å’ŒåŠŸèƒ½ç»„ä»¶
        page_pie()
    elif page == 'æŠ˜çº¿å›¾':
        page_broken()
    elif page == 'æ•£ç‚¹å›¾':
        page_point()
    elif page == 'å½±è§†æ¨è':
        page_movie()
    elif page == "å›½å†…å¤©æ°”":
        page_weather()

if __name__ == '__main__':
    main()